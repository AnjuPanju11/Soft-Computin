# ğŸ§  UNIT 2: Artificial Neural Networks (ANN)

---

## ğŸ”¹ McCulloch-Pitts Neuron Model

### ğŸ§  Basic Architecture:

McCulloch-Pitts neuron ek basic model hai jisme inputs, weights (generally +1/-1), ek threshold aur ek binary output hota hai.

### âœ… Formula:

```
Y = 1, agar Î£(wi Ã— xi) â‰¥ Î¸
Y = 0, agar Î£(wi Ã— xi) < Î¸
```

### ğŸ”§ Realization: 3-Input NAND Gate

* Inputs: X1, X2, X3
* NAND ka matlab: agar sab 1 honge, tabhi output 0 hoga, baaki sab me output 1
* Weight (w) = -1 for all inputs
* Threshold (Î¸) = -2.5

```
Î£ = -(X1 + X2 + X3) >= -2.5 â†’ output = 1
Agar X1 = X2 = X3 = 1,
Î£ = -3 < -2.5 â†’ output = 0 âœ…
```

### ğŸ”§ Realization: 3-Input NOR Gate

* Inputs: X1, X2, X3
* NOR ka matlab: sab inputs 0 hon tabhi output 1, warna 0
* Weight = -1 for all inputs
* Threshold (Î¸) = -0.5

```
Î£ = -(X1 + X2 + X3) >= -0.5
Agar X1=X2=X3=0,
Î£ = 0 â‰¥ -0.5 â†’ output = 1 âœ…
```

---

## ğŸ”¹ Activation Functions

### ğŸ’¡ Activation function ka kaam:

Ye decide karta hai ki neuron active hoga ya nahi. Har function ka output style alag hota hai.

| Function | Range    | Graph Shape | Use Case                         |
| -------- | -------- | ----------- | -------------------------------- |
| Step     | 0 or 1   | Threshold   | Simple binary classification     |
| Sigmoid  | 0 to 1   | S-shaped    | Smooth binary outputs            |
| Tanh     | -1 to +1 | S-shaped    | Centered output, faster learning |
| ReLU     | 0 to âˆ   | Linear+     | Deep learning (fast & simple)    |

---

## ğŸ”¹ Importance of Activation Functions

* Complex patterns learn karne ke liye non-linearity zaroori hai.
* Without activation, NN becomes just a linear model.
* Learning curve smooth hota hai with sigmoid/tanh
* ReLU fast train karta hai deep layers

---

## ğŸ”¹ Weight Update Formula (Backpropagation)

### ğŸ” Formula:

```
Î”w = Î· Ã— error Ã— input
```

Alternate detailed form:

```
Î”w_ij = Î· Ã— Î´_j Ã— o_i
```

* Î· = Learning rate
* Î´\_j = Error term for output neuron j
* o\_i = Output from neuron i

Backpropagation me weights backward adjust hote hain taaki error minimum ho jaye.

---

## ğŸ”¹ Architecture of Backpropagation Network

### Layers:

1. **Input Layer** â€“ Features
2. **Hidden Layer(s)** â€“ Pattern recognize karta hai
3. **Output Layer** â€“ Final prediction

### Training Steps:

1. Forward pass â€“ inputs se output generate
2. Error calculate â€“ actual vs predicted
3. Backward pass â€“ error ko back spread karo
4. Weight update â€“ formula se adjustment karo

---

## ğŸ”¹ Perceptron Training (With & Without Bias)

### ğŸ§  What is Perceptron?

Ek simple NN jisme 1 output neuron hota hai. Linear classifier hota hai.

### âœ… Training Rule:

```
w_new = w_old + Î· Ã— (target - output) Ã— input
```

### âš™ï¸ With Bias:

Bias ek extra input hota hai jiska weight bhi update hota hai.
Bias help karta hai decision boundary ko shift karne me.

### âš™ï¸ Without Bias:

Output only depends on weighted sum of inputs.

### Example (AND gate):

```
Inputs: X1, X2 | Target: X1 AND X2
```

Initial weights = 0, Learning rate = 1 â†’ update karte jao till correct output

---

## ğŸ”¹ Learning Types

### 1ï¸âƒ£ Supervised Learning:

* Input ke saath correct output diya jata hai
* Example: Email â†’ Spam or Not Spam

### 2ï¸âƒ£ Unsupervised Learning:

* Sirf input hota hai, no output
* Goal: Grouping/Clustering
* Example: Market segmentation

### 3ï¸âƒ£ Incremental Learning:

* Data gradually milta hai
* Model ko baar-baar retrain nahi karna padta
* Example: AI games jisme model game khelte-khelte smarter hota hai

---
